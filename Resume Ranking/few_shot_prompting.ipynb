{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(train_file, test_file):\n",
    "    # Load train and test datasets\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "    \n",
    "    # Prepare the data in the required format\n",
    "    train_data['winner'] = np.where(train_data['winnerId'] == train_data['candidateAId'], 'A', 'B')\n",
    "    test_data['winner'] = np.where(test_data['winnerId'] == test_data['candidateAId'], 'A', 'B')\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def create_prompt(role, candidateA, candidateB, examples):\n",
    "    prompt = f\"Role: {role}\\n\\n\"\n",
    "    prompt += \"Examples:\\n\"\n",
    "    for _, example in examples.iterrows():\n",
    "        prompt += f\"Candidate A: {example['candidateATranscript']}\\n\"\n",
    "        prompt += f\"Candidate B: {example['candidateBTranscript']}\\n\"\n",
    "        prompt += f\"Better candidate: {example['winner']}\\n\\n\"\n",
    "    prompt += f\"Candidate A: {candidateA}\\n\"\n",
    "    prompt += f\"Candidate B: {candidateB}\\n\"\n",
    "    prompt += \"Better candidate: \"\n",
    "    return prompt\n",
    "\n",
    "def get_embedding(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token embedding as the sentence representation\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "def few_shot_prediction(model, tokenizer, role, candidateA, candidateB, examples):\n",
    "    prompt = create_prompt(role, candidateA, candidateB, examples)\n",
    "    \n",
    "    # Get embeddings for the entire prompt\n",
    "    prompt_embedding = get_embedding(model, tokenizer, prompt)\n",
    "    \n",
    "    # Get embeddings for \"Candidate A\" and \"Candidate B\"\n",
    "    embedding_A = get_embedding(model, tokenizer, f\"Candidate A: {candidateA}\")\n",
    "    embedding_B = get_embedding(model, tokenizer, f\"Candidate B: {candidateB}\")\n",
    "    \n",
    "    # Compare cosine similarities\n",
    "    similarity_A = cosine_similarity([prompt_embedding], [embedding_A])[0][0]\n",
    "    similarity_B = cosine_similarity([prompt_embedding], [embedding_B])[0][0]\n",
    "    \n",
    "    return \"A\" if similarity_A > similarity_B else \"B\"\n",
    "\n",
    "def main():\n",
    "    # Load BERT model and tokenizer\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Load your dataset\n",
    "    train_data, test_data = load_dataset('train_dataset.csv', 'test_dataset.csv')\n",
    "\n",
    "    # Select a few examples for few-shot learning\n",
    "    few_shot_examples = train_data.sample(n=5, random_state=42)  # Adjust the number of examples as needed\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for _, sample in test_data.iterrows():\n",
    "        prediction = few_shot_prediction(\n",
    "            model, tokenizer,\n",
    "            sample['role'],\n",
    "            sample['candidateATranscript'],\n",
    "            sample['candidateBTranscript'],\n",
    "            few_shot_examples\n",
    "        )\n",
    "        \n",
    "        correct_predictions += (prediction == sample['winner'])\n",
    "        total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Few-shot learning accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
