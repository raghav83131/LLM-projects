{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidateAId</th>\n",
       "      <th>candidateBId</th>\n",
       "      <th>winnerId</th>\n",
       "      <th>candidateATranscript</th>\n",
       "      <th>candidateBTranscript</th>\n",
       "      <th>candidateAResume</th>\n",
       "      <th>candidateBResume</th>\n",
       "      <th>role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ab47434-09a9-44e6-8c77-f9fd20c57765</td>\n",
       "      <td>d7cbd002-5423-4dae-82d9-3a629ec361bb</td>\n",
       "      <td>8ab47434-09a9-44e6-8c77-f9fd20c57765</td>\n",
       "      <td>{'pairs': [['Interviewer: Hello and welcome to...</td>\n",
       "      <td>{'pairs': [['Interviewer: Hello and welcome to...</td>\n",
       "      <td>{\"data\": {\"awards\": [], \"certifications\": [], ...</td>\n",
       "      <td>{\"data\": {\"awards\": [], \"certifications\": [], ...</td>\n",
       "      <td>communications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53c11bf9-3ec7-4909-a9d1-487692e72415</td>\n",
       "      <td>e957aff1-583b-11ef-8a84-4201ac164110</td>\n",
       "      <td>e957aff1-583b-11ef-8a84-4201ac164110</td>\n",
       "      <td>{'pairs': [['Interviewer: Hello and welcome to...</td>\n",
       "      <td>{'pairs': [['Interviewer: Hello! This is a sho...</td>\n",
       "      <td>{\"data\": {\"awards\": [], \"certifications\": [], ...</td>\n",
       "      <td>{\"data\": {\"awards\": [], \"certifications\": [], ...</td>\n",
       "      <td>ops-or-gtm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4617b14d-ca26-11ee-a4ba-42010a400021</td>\n",
       "      <td>a2d2933e-c5bb-11ee-a4ba-42010a400021</td>\n",
       "      <td>4617b14d-ca26-11ee-a4ba-42010a400021</td>\n",
       "      <td>{'pairs': [['Interviewer: Hello and welcome to...</td>\n",
       "      <td>{'pairs': [['Interviewer: Hello and welcome to...</td>\n",
       "      <td>{\"data\": {\"awards\": [\"1st (Winner) AIR 8\", \"Am...</td>\n",
       "      <td>{\"data\": {\"awards\": [], \"certifications\": [\"In...</td>\n",
       "      <td>has-scraping-experience-a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           candidateAId                          candidateBId  \\\n",
       "0  8ab47434-09a9-44e6-8c77-f9fd20c57765  d7cbd002-5423-4dae-82d9-3a629ec361bb   \n",
       "1  53c11bf9-3ec7-4909-a9d1-487692e72415  e957aff1-583b-11ef-8a84-4201ac164110   \n",
       "2  4617b14d-ca26-11ee-a4ba-42010a400021  a2d2933e-c5bb-11ee-a4ba-42010a400021   \n",
       "\n",
       "                               winnerId  \\\n",
       "0  8ab47434-09a9-44e6-8c77-f9fd20c57765   \n",
       "1  e957aff1-583b-11ef-8a84-4201ac164110   \n",
       "2  4617b14d-ca26-11ee-a4ba-42010a400021   \n",
       "\n",
       "                                candidateATranscript  \\\n",
       "0  {'pairs': [['Interviewer: Hello and welcome to...   \n",
       "1  {'pairs': [['Interviewer: Hello and welcome to...   \n",
       "2  {'pairs': [['Interviewer: Hello and welcome to...   \n",
       "\n",
       "                                candidateBTranscript  \\\n",
       "0  {'pairs': [['Interviewer: Hello and welcome to...   \n",
       "1  {'pairs': [['Interviewer: Hello! This is a sho...   \n",
       "2  {'pairs': [['Interviewer: Hello and welcome to...   \n",
       "\n",
       "                                    candidateAResume  \\\n",
       "0  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
       "1  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
       "2  {\"data\": {\"awards\": [\"1st (Winner) AIR 8\", \"Am...   \n",
       "\n",
       "                                    candidateBResume  \\\n",
       "0  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
       "1  {\"data\": {\"awards\": [], \"certifications\": [], ...   \n",
       "2  {\"data\": {\"awards\": [], \"certifications\": [\"In...   \n",
       "\n",
       "                        role  \n",
       "0             communications  \n",
       "1                 ops-or-gtm  \n",
       "2  has-scraping-experience-a  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_dataset.csv')\n",
    "test_df = pd.read_csv('test_dataset.csv')\n",
    "\n",
    "# Display the first few rows of the training dataset\n",
    "print(\"Training Data:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kavin Kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Data Preprocessing and Tokenization \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Function to clean text (Transcript)\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary special characters\n",
    "    return re.sub(r'[^A-Za-z0-9\\s.,!?]', '', text)\n",
    "\n",
    "# Function to convert Resume dictionary to text\n",
    "def dict_to_text(Resume):\n",
    "    if isinstance(Resume, str):\n",
    "        try:\n",
    "            # If Resume is a string, try converting it to a dictionary\n",
    "            Resume = eval(Resume)  # Be cautious with eval; json.loads is safer for JSON input\n",
    "        except:\n",
    "            pass  # If conversion fails, we'll keep it as a string\n",
    "    if isinstance(Resume, dict):\n",
    "        Resume_text = \"\"\n",
    "        for key, value in Resume.items():\n",
    "            if isinstance(value, list):\n",
    "                value = \", \".join(value)\n",
    "            Resume_text += f\"{key}: {value}. \"\n",
    "        return Resume_text\n",
    "    return str(Resume)  # Return the string if it's not a dict\n",
    "\n",
    "# Function to preprocess a single row\n",
    "def preprocess_row(row):\n",
    "    # Clean the Transcript text\n",
    "    candidateATranscript_clean = clean_text(row['candidateATranscript'])\n",
    "    candidateBTranscript_clean = clean_text(row['candidateBTranscript'])\n",
    "\n",
    "    # Convert the Resume dictionaries to text\n",
    "    candidateAResume_text = dict_to_text(row['candidateAResume'])\n",
    "    candidateBResume_text = dict_to_text(row['candidateBResume'])\n",
    "\n",
    "    # Concatenate Transcript and Resume for both candidates\n",
    "    candidateA_data = candidateATranscript_clean + \" \" + candidateAResume_text\n",
    "    candidateB_data = candidateBTranscript_clean + \" \" + candidateBResume_text\n",
    "\n",
    "    # Combine into a tuple for further processing\n",
    "    return candidateA_data, candidateB_data, row['role']\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize the preprocessed data\n",
    "def preprocess_data(dataframe):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in dataframe.iterrows():\n",
    "        # Preprocess each row\n",
    "        candidateA_data, candidateB_data, role = preprocess_row(row)\n",
    "\n",
    "        # Combine both candidates' text with a separator token and the role as context\n",
    "        combined_text = role + \" [SEP] \" + candidateA_data + \" [SEP] \" + candidateB_data\n",
    "\n",
    "        # Tokenize the combined input\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "            combined_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        inputs.append(encoded_input)\n",
    "        labels.append(1 if row['winnerId'] == row['candidateAId'] else 0)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "# Apply the preprocessing to the training and test datasets\n",
    "train_inputs, train_labels = preprocess_data(train_df)\n",
    "test_inputs, test_labels = preprocess_data(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### Creating Data Loaders\n",
    "\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Use GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "if torch.backends.mps.is_available():  # Checks for AMD GPU with ROCm support\n",
    "        device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Convert inputs to TensorDataset\n",
    "def create_tensor_dataset(inputs, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for encoded_input in inputs:\n",
    "        input_ids.append(encoded_input['input_ids'])\n",
    "        attention_masks.append(encoded_input['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_dataset = create_tensor_dataset(train_inputs, train_labels)\n",
    "test_dataset = create_tensor_dataset(test_inputs, test_labels)\n",
    "\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set up the training parameters\n",
    "epochs = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:28<00:00,  9.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7021889090538025\n",
      "Training accuracy: 0.4\n",
      "Test accuracy: 0.5853658536585366\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:29<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6723439693450928\n",
      "Training accuracy: 0.6\n",
      "Test accuracy: 0.5914634146341463\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:28<00:00,  9.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6514999270439148\n",
      "Training accuracy: 0.7\n",
      "Test accuracy: 0.4695121951219512\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:28<00:00,  9.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6580226421356201\n",
      "Training accuracy: 0.6\n",
      "Test accuracy: 0.5060975609756098\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:28<00:00,  9.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6133987108866373\n",
      "Training accuracy: 0.75\n",
      "Test accuracy: 0.6097560975609756\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:28<00:00,  9.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6637960076332092\n",
      "Training accuracy: 0.65\n",
      "Test accuracy: 0.573170731707317\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:30<00:00, 10.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6260787049929301\n",
      "Training accuracy: 0.75\n",
      "Test accuracy: 0.6158536585365854\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:28<00:00,  9.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5559169252713522\n",
      "Training accuracy: 0.75\n",
      "Test accuracy: 0.5\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:27<00:00,  9.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5738959312438965\n",
      "Training accuracy: 0.75\n",
      "Test accuracy: 0.6158536585365854\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:28<00:00,  9.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5122632086277008\n",
      "Training accuracy: 0.8\n",
      "Test accuracy: 0.6036585365853658\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Clear gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions.double() / len(train_dataset)\n",
    "\n",
    "    print(f\"Training loss: {avg_train_loss}\")\n",
    "    print(f\"Training accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_masks)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    test_accuracy = correct_predictions.double() / len(test_dataset)\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
